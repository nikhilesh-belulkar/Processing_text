{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"mMoD_7WYNXkC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591386991256,"user_tz":240,"elapsed":794161,"user":{"displayName":"Earl Hsiieh","photoUrl":"","userId":"02783385986222153971"}},"outputId":"51d0b794-e0e1-4633-ddbf-b995e7aa7ad2"},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat May 23 16:54:55 2020\n","\n","@author: Nick\n","\"\"\"\n","import numpy as np\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.lancaster import LancasterStemmer\n","import nltk\n","import re\n","from sklearn.preprocessing import OneHotEncoder\n","import matplotlib.pyplot as plt\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n","from keras.callbacks import ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","import re \n","from functions import *\n","\n","intent, unique_intent, sentences = load_dataset(\"data3.csv\")\n","\n","print(sentences[:5])\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")\n","\n","for i in range(len(sentences)):\n","  sentences[i] = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(sentences[i]), flags=re.MULTILINE)\n","\n","cleaned_words = cleaning(sentences)   \n","stemmer = LancasterStemmer()\n","\n","\n","word_tokenizer = create_tokenizer(cleaned_words)\n","vocab_size = len(word_tokenizer.word_index) + 1\n","max_length = max_length(cleaned_words)\n","\n","print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))\n","\n","encoded_doc = encoding_doc(word_tokenizer, cleaned_words)\n","padded_doc = padding_doc(encoded_doc, max_length)\n","padded_doc[:5]\n","output_tokenizer = create_tokenizer(unique_intent, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n","output_tokenizer.word_index\n","encoded_output = encoding_doc(output_tokenizer, intent)\n","encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)\n","\n","encoded_output.shape\n","\n","output_one_hot = one_hot(encoded_output)\n","\n","\n","\n","\n","\n"," \n","def predictions(text):\n","  clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n","  test_word = word_tokenize(clean)\n","  test_word = [w.lower() for w in test_word]\n","  test_ls = word_tokenizer.texts_to_sequences(test_word)\n","  print(test_word)\n","  #Check for unknown words\n","  if [] in test_ls:\n","    test_ls = list(filter(None, test_ls))\n","    \n","  test_ls = np.array(test_ls).reshape(1, len(test_ls))\n"," \n","  x = padding_doc(test_ls, max_length)\n","  \n","  pred = model.predict_proba(x)\n","  \n","  \n","  return pred\n","\n","def get_final_output(pred, classes):\n","  predictions = pred[0]\n"," \n","  classes = np.array(classes)\n","  ids = np.argsort(-predictions)\n","  classes = classes[ids]\n","  predictions = -np.sort(-predictions)\n"," \n","  for i in range(pred.shape[1]):\n","    print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n","\n","\n","\n","train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.1)\n","print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n","print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))\n","\n","\n","model = create_model(vocab_size, max_length)\n","\n","model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n","model.summary()\n","\n","filename = 'model.h5'\n","checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","\n","hist = model.fit(train_X, train_Y, epochs=42 , batch_size = 15, validation_data = (val_X, val_Y), callbacks = [checkpoint])\n","\n","\n","\n","model = load_model(\"model.h5\")\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                            Sentence Intent\n","0  Hi Criag,\\r\\n\\r\\nThank you for reaching out.  ...   pass\n","1                     Thanks Craig\\r\\nWe will pass f   pass\n","2  HI Craig \\xe2\\x80\\x93 this one is a pass for m...   pass\n","3  Hi Craig,\\r\\n\\r\\nIt\\xe2\\x80\\x99s nice to meet ...   pass\n","4  Appreciate heads up on it, I will need to pass...   pass\n","[\"Hi Criag,\\\\r\\\\n\\\\r\\\\nThank you for reaching out.  However, this opportunity is completely\\\\r\\\\noutside our fund's focus on consumer brands, thus we will pass.\\\\r\\\\n\\\\r\\\\nBest of luck with the closing!\\\\r\\\\n\\\\r\\\\nSarah\\\\r\\\\n\\\\xe1\\\\x90\\\\xa7\\\\r\\\\n\\\\r\\\\n\", 'Thanks Craig\\\\r\\\\nWe will pass f', 'HI Craig \\\\xe2\\\\x80\\\\x93 this one is a pass for me, outside my knowledge base', 'Hi Craig,\\\\r\\\\n\\\\r\\\\nIt\\\\xe2\\\\x80\\\\x99s nice to meet you.  Unfortunately, we\\\\xe2\\\\x80\\\\x99ll pass on this.  I hope you stay\\\\r\\\\nsafe and healthy.\\\\r\\\\n\\\\r\\\\nBest regards,\\\\r\\\\nJohn\\\\r\\\\n\\\\r\\\\nO', 'Appreciate heads up on it, I will need to pass on this one.  Thx\\\\r\\\\n\\\\r\\\\nIan Welch\\\\r\\\\nMenlo Park, CA\\\\r\\\\']\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Vocab Size = 3482 and Maximum length = 564\n","Shape of train_X = (286, 564) and train_Y = (286, 2)\n","Shape of val_X = (32, 564) and val_Y = (32, 2)\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 564, 128)          445696    \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 256)               263168    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 32)                8224      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 32)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2)                 66        \n","=================================================================\n","Total params: 717,154\n","Trainable params: 271,458\n","Non-trainable params: 445,696\n","_________________________________________________________________\n","Train on 286 samples, validate on 32 samples\n","Epoch 1/42\n","286/286 [==============================] - 20s 69ms/step - loss: 0.6503 - accuracy: 0.6084 - val_loss: 0.6714 - val_accuracy: 0.6250\n","\n","Epoch 00001: val_loss improved from inf to 0.67143, saving model to model.h5\n","Epoch 2/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.6476 - accuracy: 0.6643 - val_loss: 0.6446 - val_accuracy: 0.6250\n","\n","Epoch 00002: val_loss improved from 0.67143 to 0.64457, saving model to model.h5\n","Epoch 3/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.6340 - accuracy: 0.6818 - val_loss: 0.6136 - val_accuracy: 0.6250\n","\n","Epoch 00003: val_loss improved from 0.64457 to 0.61361, saving model to model.h5\n","Epoch 4/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.5890 - accuracy: 0.6783 - val_loss: 0.6158 - val_accuracy: 0.6250\n","\n","Epoch 00004: val_loss did not improve from 0.61361\n","Epoch 5/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.4923 - accuracy: 0.7867 - val_loss: 0.5525 - val_accuracy: 0.7500\n","\n","Epoch 00005: val_loss improved from 0.61361 to 0.55247, saving model to model.h5\n","Epoch 6/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.4548 - accuracy: 0.8077 - val_loss: 0.5232 - val_accuracy: 0.7188\n","\n","Epoch 00006: val_loss improved from 0.55247 to 0.52322, saving model to model.h5\n","Epoch 7/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.3686 - accuracy: 0.8287 - val_loss: 0.5025 - val_accuracy: 0.7188\n","\n","Epoch 00007: val_loss improved from 0.52322 to 0.50248, saving model to model.h5\n","Epoch 8/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.3292 - accuracy: 0.8811 - val_loss: 0.5668 - val_accuracy: 0.7500\n","\n","Epoch 00008: val_loss did not improve from 0.50248\n","Epoch 9/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.4832 - accuracy: 0.7587 - val_loss: 0.5799 - val_accuracy: 0.7188\n","\n","Epoch 00009: val_loss did not improve from 0.50248\n","Epoch 10/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.3665 - accuracy: 0.8636 - val_loss: 0.5274 - val_accuracy: 0.7500\n","\n","Epoch 00010: val_loss did not improve from 0.50248\n","Epoch 11/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.2923 - accuracy: 0.8776 - val_loss: 0.5246 - val_accuracy: 0.7188\n","\n","Epoch 00011: val_loss did not improve from 0.50248\n","Epoch 12/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.2613 - accuracy: 0.9126 - val_loss: 0.3983 - val_accuracy: 0.8438\n","\n","Epoch 00012: val_loss improved from 0.50248 to 0.39830, saving model to model.h5\n","Epoch 13/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.2657 - accuracy: 0.8986 - val_loss: 0.5265 - val_accuracy: 0.7500\n","\n","Epoch 00013: val_loss did not improve from 0.39830\n","Epoch 14/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.2479 - accuracy: 0.9021 - val_loss: 0.5493 - val_accuracy: 0.7500\n","\n","Epoch 00014: val_loss did not improve from 0.39830\n","Epoch 15/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.2055 - accuracy: 0.9196 - val_loss: 0.6401 - val_accuracy: 0.7500\n","\n","Epoch 00015: val_loss did not improve from 0.39830\n","Epoch 16/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.2398 - accuracy: 0.9301 - val_loss: 0.6427 - val_accuracy: 0.6562\n","\n","Epoch 00016: val_loss did not improve from 0.39830\n","Epoch 17/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.2677 - accuracy: 0.9056 - val_loss: 0.4803 - val_accuracy: 0.7500\n","\n","Epoch 00017: val_loss did not improve from 0.39830\n","Epoch 18/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.1521 - accuracy: 0.9510 - val_loss: 0.5655 - val_accuracy: 0.8125\n","\n","Epoch 00018: val_loss did not improve from 0.39830\n","Epoch 19/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.1171 - accuracy: 0.9685 - val_loss: 0.7416 - val_accuracy: 0.7812\n","\n","Epoch 00019: val_loss did not improve from 0.39830\n","Epoch 20/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0936 - accuracy: 0.9685 - val_loss: 0.6286 - val_accuracy: 0.7188\n","\n","Epoch 00020: val_loss did not improve from 0.39830\n","Epoch 21/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0950 - accuracy: 0.9790 - val_loss: 0.6462 - val_accuracy: 0.7500\n","\n","Epoch 00021: val_loss did not improve from 0.39830\n","Epoch 22/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0595 - accuracy: 0.9825 - val_loss: 0.6304 - val_accuracy: 0.8125\n","\n","Epoch 00022: val_loss did not improve from 0.39830\n","Epoch 23/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0561 - accuracy: 0.9825 - val_loss: 0.7854 - val_accuracy: 0.7500\n","\n","Epoch 00023: val_loss did not improve from 0.39830\n","Epoch 24/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.1573 - accuracy: 0.9301 - val_loss: 0.8065 - val_accuracy: 0.6875\n","\n","Epoch 00024: val_loss did not improve from 0.39830\n","Epoch 25/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.2364 - accuracy: 0.9021 - val_loss: 0.6512 - val_accuracy: 0.8125\n","\n","Epoch 00025: val_loss did not improve from 0.39830\n","Epoch 26/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.1990 - accuracy: 0.9266 - val_loss: 0.6274 - val_accuracy: 0.6875\n","\n","Epoch 00026: val_loss did not improve from 0.39830\n","Epoch 27/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.1637 - accuracy: 0.9545 - val_loss: 0.6271 - val_accuracy: 0.7812\n","\n","Epoch 00027: val_loss did not improve from 0.39830\n","Epoch 28/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0876 - accuracy: 0.9790 - val_loss: 0.6139 - val_accuracy: 0.7188\n","\n","Epoch 00028: val_loss did not improve from 0.39830\n","Epoch 29/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0882 - accuracy: 0.9685 - val_loss: 0.7261 - val_accuracy: 0.7500\n","\n","Epoch 00029: val_loss did not improve from 0.39830\n","Epoch 30/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0560 - accuracy: 0.9860 - val_loss: 0.6826 - val_accuracy: 0.7812\n","\n","Epoch 00030: val_loss did not improve from 0.39830\n","Epoch 31/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.0345 - accuracy: 0.9930 - val_loss: 0.7729 - val_accuracy: 0.8125\n","\n","Epoch 00031: val_loss did not improve from 0.39830\n","Epoch 32/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0368 - accuracy: 0.9790 - val_loss: 0.9018 - val_accuracy: 0.7500\n","\n","Epoch 00032: val_loss did not improve from 0.39830\n","Epoch 33/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.1539 - accuracy: 0.9336 - val_loss: 0.6622 - val_accuracy: 0.7812\n","\n","Epoch 00033: val_loss did not improve from 0.39830\n","Epoch 34/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.0791 - accuracy: 0.9790 - val_loss: 0.5871 - val_accuracy: 0.8125\n","\n","Epoch 00034: val_loss did not improve from 0.39830\n","Epoch 35/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0410 - accuracy: 0.9895 - val_loss: 0.6509 - val_accuracy: 0.8125\n","\n","Epoch 00035: val_loss did not improve from 0.39830\n","Epoch 36/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0258 - accuracy: 0.9895 - val_loss: 0.6895 - val_accuracy: 0.8438\n","\n","Epoch 00036: val_loss did not improve from 0.39830\n","Epoch 37/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.0277 - accuracy: 0.9930 - val_loss: 0.7367 - val_accuracy: 0.8750\n","\n","Epoch 00037: val_loss did not improve from 0.39830\n","Epoch 38/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.7667 - val_accuracy: 0.8438\n","\n","Epoch 00038: val_loss did not improve from 0.39830\n","Epoch 39/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.0308 - accuracy: 0.9965 - val_loss: 0.7836 - val_accuracy: 0.8750\n","\n","Epoch 00039: val_loss did not improve from 0.39830\n","Epoch 40/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.0368 - accuracy: 0.9860 - val_loss: 0.8195 - val_accuracy: 0.8438\n","\n","Epoch 00040: val_loss did not improve from 0.39830\n","Epoch 41/42\n","286/286 [==============================] - 19s 65ms/step - loss: 0.0296 - accuracy: 0.9860 - val_loss: 0.7665 - val_accuracy: 0.8125\n","\n","Epoch 00041: val_loss did not improve from 0.39830\n","Epoch 42/42\n","286/286 [==============================] - 19s 66ms/step - loss: 0.0348 - accuracy: 0.9895 - val_loss: 0.7730 - val_accuracy: 0.8438\n","\n","Epoch 00042: val_loss did not improve from 0.39830\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eFcbF7t0eEH4","colab_type":"code","colab":{}},"source":["def get_final(pred, classes):\n","  predictions = pred[0]\n","  classes = np.array(classes)\n","  ids = np.argsort(-predictions)\n","  classes = classes[ids]\n","  predictions = -np.sort(-predictions)\n","  return classes[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"udWHd99M_96f","colab_type":"code","colab":{}},"source":["def predictions(text):\n","  clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n","  test_word = word_tokenize(clean)\n","  test_word = [w.lower() for w in test_word]\n","  test_ls = word_tokenizer.texts_to_sequences(test_word)\n","  print(test_word)\n","  #Check for unknown words\n","  if [] in test_ls:\n","    test_ls = list(filter(None, test_ls))\n","    \n","  test_ls = np.array(test_ls).reshape(1, len(test_ls))\n"," \n","  x = padding_doc(test_ls, max_length)\n","  \n","  pred = model.predict_proba(x)\n","  \n","  \n","  return pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJ2-JCP8M_Xb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1591386002805,"user_tz":240,"elapsed":664,"user":{"displayName":"Earl Hsiieh","photoUrl":"","userId":"02783385986222153971"}},"outputId":"d2b9fb6f-d29b-487a-a94a-0b2495aebddb"},"source":["text = \"\"\"Hey Mate, \\r\\n\\r\\n\\r\\nI\\xe2\\x80\\x99m really interested Craig, let\\xe2\\x80\\x99s set up a time to meet up and discu\"\"\"\n","pred = predictions(text)\n","get_final_output(pred, unique_intent)\n","get_final(pred, unique_intent)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-be9606caa808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"Hey Mate, \\r\\n\\r\\n\\r\\nI\\xe2\\x80\\x99m really interested Craig, let\\xe2\\x80\\x99s set up a time to meet up and discu\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mget_final_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_intent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_intent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"]}]},{"cell_type":"code","metadata":{"id":"tU0JxaN6cesM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1591387228606,"user_tz":240,"elapsed":1185,"user":{"displayName":"Earl Hsiieh","photoUrl":"","userId":"02783385986222153971"}},"outputId":"f85fd992-2b3b-4687-f69d-afb2bcd06c4b"},"source":["##Creating a csv file with the interested/pass values for every email \n","import csv \n","\n","f  = open(\"ID_cmckeon@cbmpartners.csv\")\n","csv_f  = csv.reader(f)\n","classifications = {}\n","\n","\n","def add_all(a):\n","  string = ''\n","  for i in range(len(a)):\n","    string = string+a[i]\n","  return string\n","\n","\n","\n","for row in csv_f:\n","  emailnum = row[-1]\n","  print(\"Email-num:\")\n","  print(emailnum)\n","  print(\"Dingo\")\n","  msg = row[1]\n","  pre  = predictions(msg)\n","  classifications[emailnum] = get_final(pre, unique_intent)\n","\n","\n","filename = \"CLASSIFICATIONS_CMCKEON@CBMPARTNERS.csv\"\n","with open(filename,'w') as document:\n","  document.write(\"Email, Classification\\n\")\n","  for key in classifications.keys():\n","    document.write(\"%s,%s\\n\"%(key,classifications[key]))\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Email-num:\n","jibreel@petramdata.com\n","Dingo\n","Things are exceedingly busy for me and my team right now.  I\\xe2\\x80\\x99d suggest you\\r\\nmay want to talk to my business partner Dhani Jones (cc\\xe2\\x80\\x99d here).\\r\\n\\r\\n\\r\\n\\r\\nJibreel\\r\\n\\r\\n\\r\\n\\r\\n*From:* Craig mckeon \n","['things', 'are', 'exceedingly', 'busy', 'for', 'me', 'and', 'my', 'team', 'right', 'now', 'i', 'xe2', 'x80', 'x99d', 'suggest', 'you', 'r', 'nmay', 'want', 'to', 'talk', 'to', 'my', 'business', 'partner', 'dhani', 'jones', 'cc', 'xe2', 'x80', 'x99d', 'here', 'r', 'n', 'r', 'n', 'r', 'n', 'r', 'njibreel', 'r', 'n', 'r', 'n', 'r', 'n', 'r', 'n', 'from', 'craig', 'mckeon']\n"],"name":"stdout"}]}]}